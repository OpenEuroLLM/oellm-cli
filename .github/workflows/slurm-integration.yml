name: SLURM Integration Test

on:
  workflow_dispatch:
  pull_request:
    branches: [main]
    paths:
      - 'oellm/**'
      - 'tests/integration/**'
      - '.github/workflows/slurm-integration.yml'
      - 'apptainer/slurm-ci.def'

jobs:
  slurm-integration:
    name: SLURM + Apptainer Integration Test (GPU)
    runs-on: "runs-on=${{ github.run_id }}/family=g5.xlarge/image=ubuntu24-gpu-x64/spot=false"
    timeout-minutes: 45

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check GPU availability
        run: |
          echo "Checking for NVIDIA GPU..."
          nvidia-smi
          echo "GPU check passed"
          echo "Disk info:"
          df -h

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            software-properties-common \
            munge \
            slurm-wlm \
            slurm-wlm-basic-plugins \
            libmunge-dev

      - name: Install Apptainer
        run: |
          sudo add-apt-repository -y ppa:apptainer/ppa
          sudo apt-get update
          sudo apt-get install -y apptainer

      - name: Configure MUNGE
        run: |
          # Create munge key manually (create-munge-key not available on Ubuntu)
          sudo dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024
          sudo chown munge:munge /etc/munge/munge.key
          sudo chmod 400 /etc/munge/munge.key
          sudo systemctl start munge
          sudo systemctl status munge

      - name: Configure SLURM
        run: |
          HOSTNAME=$(hostname -s)

          sudo mkdir -p /etc/slurm
          sudo mkdir -p /var/spool/slurmd
          sudo mkdir -p /var/spool/slurmctld
          sudo mkdir -p /var/log/slurm

          # Create slurm.conf with GPU support
          sudo tee /etc/slurm/slurm.conf > /dev/null << EOF
          ClusterName=ci
          SlurmctldHost=${HOSTNAME}
          MpiDefault=none
          ProctrackType=proctrack/linuxproc
          ReturnToService=2
          SlurmctldPidFile=/var/run/slurmctld.pid
          SlurmdPidFile=/var/run/slurmd.pid
          SlurmdSpoolDir=/var/spool/slurmd
          SlurmUser=root
          StateSaveLocation=/var/spool/slurmctld
          SwitchType=switch/none
          TaskPlugin=task/none

          # Scheduling
          SchedulerType=sched/backfill
          SelectType=select/cons_tres
          SelectTypeParameters=CR_Core

          # Logging
          SlurmctldLogFile=/var/log/slurm/slurmctld.log
          SlurmdLogFile=/var/log/slurm/slurmd.log

          # GRES (GPU) configuration
          GresTypes=gpu

          # Node definition with GPU (use 15000 to account for system overhead on g4dn.xlarge)
          NodeName=${HOSTNAME} CPUs=4 Gres=gpu:1 RealMemory=15000 State=UNKNOWN
          PartitionName=gpu Nodes=${HOSTNAME} Default=YES MaxTime=INFINITE State=UP
          EOF

          # Create gres.conf for GPU
          sudo tee /etc/slurm/gres.conf > /dev/null << EOF
          NodeName=${HOSTNAME} Name=gpu File=/dev/nvidia0
          EOF

          # Set permissions
          sudo chmod 644 /etc/slurm/slurm.conf
          sudo chmod 644 /etc/slurm/gres.conf

      - name: Start SLURM services
        run: |
          sudo slurmctld -D &
          sleep 3
          sudo slurmd -D &
          sleep 3

          # Wait for node to be ready
          for i in {1..30}; do
            if sinfo | grep -q "idle"; then
              echo "SLURM node is ready"
              break
            fi
            echo "Waiting for SLURM node to become idle... ($i/30)"
            sleep 2
          done

          sinfo
          scontrol show node

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          version: "latest"

      - name: Set up Python 3.12
        run: uv python install 3.12

      - name: Install oellm with dev dependencies
        run: uv sync --extra dev

      - name: Set up test environment
        run: |
          # Use a directory next to the workspace (which runs-on puts on NVMe for instance-storage types)
          EVAL_BASE_DIR="${GITHUB_WORKSPACE}/../oellm-test"
          mkdir -p "$EVAL_BASE_DIR"
          mkdir -p "$EVAL_BASE_DIR/hf_data/hub"
          mkdir -p "$EVAL_BASE_DIR/hf_data/datasets"
          mkdir -p "$EVAL_BASE_DIR/$USER"
          # Export for subsequent steps
          echo "EVAL_BASE_DIR=$EVAL_BASE_DIR" >> $GITHUB_ENV

      - name: Run integration tests with resource monitoring
        run: |
          # Print baseline
          echo "ðŸ“Š [BASELINE] Memory: $(free -h | awk '/^Mem:/{print $3"/"$2" (available: "$7")"}')"
          echo "ðŸ“Š [BASELINE] Disk: $(df -h / | awk 'NR==2{print $3"/"$2" ("$5" used)"}')"

          # Start background monitor that prints to stdout (visible in GH Actions console)
          (
            sleep 15  # First check after 15s
            while true; do
              echo ""
              echo "ðŸ“Š [MONITOR $(date +%H:%M:%S)] Memory: $(free -h | awk '/^Mem:/{print $3"/"$2}') | Disk: $(df -h / | awk 'NR==2{print $3"/"$2}')"
              # GPU monitoring
              if command -v nvidia-smi &> /dev/null; then
                echo "ðŸ“Š [GPU] $(nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits | awk -F', ' '{print $1"/"$2" MiB ("$3"% util)"}')"
              fi
              sleep 15
            done
          ) &
          MONITOR_PID=$!

          # Run tests
          uv run pytest tests/integration/test_slurm.py -v -s --tb=long --timeout=600
          TEST_EXIT=$?

          # Cleanup monitor
          kill $MONITOR_PID 2>/dev/null || true

          exit $TEST_EXIT
        continue-on-error: true
        id: flores_test

      - name: Collect logs
        if: always()
        run: |
          mkdir -p /tmp/slurm-logs
          # SLURM daemon logs
          sudo cp -r /var/log/slurm/* /tmp/slurm-logs/ 2>/dev/null || true
          sudo chmod -R 755 /tmp/slurm-logs 2>/dev/null || true
          # Only copy the evaluation run directory (logs, scripts, results), not the container or cached data
          cp -r "$EVAL_BASE_DIR/runner" /tmp/slurm-logs/ 2>/dev/null || true

          # System diagnostics
          echo "=== Final Resource State ===" > /tmp/slurm-logs/debug_info.txt
          echo "-- Memory --" >> /tmp/slurm-logs/debug_info.txt
          free -h >> /tmp/slurm-logs/debug_info.txt
          echo "-- Disk --" >> /tmp/slurm-logs/debug_info.txt
          df -h >> /tmp/slurm-logs/debug_info.txt

          echo "" >> /tmp/slurm-logs/debug_info.txt
          echo "=== OOM Killer Messages ===" >> /tmp/slurm-logs/debug_info.txt
          sudo dmesg | grep -i -E 'oom|killed|out of memory' >> /tmp/slurm-logs/debug_info.txt 2>&1 || echo "No OOM messages" >> /tmp/slurm-logs/debug_info.txt

          echo "" >> /tmp/slurm-logs/debug_info.txt
          echo "=== squeue ===" >> /tmp/slurm-logs/debug_info.txt
          squeue -a >> /tmp/slurm-logs/debug_info.txt 2>&1 || true
          echo "=== ps aux | grep -E 'python|lighteval' ===" >> /tmp/slurm-logs/debug_info.txt
          ps aux | grep -E 'python|lighteval' >> /tmp/slurm-logs/debug_info.txt 2>&1 || true
          echo "=== sacrebleu cache ===" >> /tmp/slurm-logs/debug_info.txt
          ls -la ~/.sacrebleu/models/ >> /tmp/slurm-logs/debug_info.txt 2>&1 || echo "No sacrebleu cache" >> /tmp/slurm-logs/debug_info.txt

      - name: Upload logs (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: slurm-logs-flores200-debug
          path: /tmp/slurm-logs/
          retention-days: 30

      - name: Fail if flores test failed
        if: steps.flores_test.outcome == 'failure'
        run: exit 1
